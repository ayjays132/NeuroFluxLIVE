# ğŸš€ NeuroFluxLIVE â€“ Premium Continuous Learning Pipeline

NeuroFluxLIVE fuses supervised fineâ€‘tuning, reinforcement learning, and evolutionary selfâ€‘optimization into a single **alwaysâ€‘on** workflow. The centerpiece is [`premium_workflow.py`](premium_workflow.py), exposed as the `premium_workflow` command after installation.

## âœ¨ Features
- **Unified pipeline** â€“ dataset analysis, Sproutâ€‘AGI fineâ€‘tuning, Gym RL training, and evolutionary learning all run from one script.
- **Model agnostic** â€“ drop in any Hugging Face causal model (GPTâ€‘2, LLaMA, Qwen, etc.).
- **Continuous adaptation** â€“ `evolutionary_learner.py` tracks performance and mutates hyperâ€‘parameters while the model serves requests.
- **Crashâ€‘safe checkpoints** â€“ tuned weights land in `sprout_benchmark/best_model` only when perplexity improves.

## ğŸ”§ Installation
```bash
pip install -r requirements.txt
pip install -e .
```

## ğŸš€ Quick Start
Run the full showcase pipeline:
```bash
premium_workflow
```

### ğŸ”¬ Sproutâ€‘AGI Benchmark
Evaluate **ayjays132/NeuroReasoner-1-NR-1** on the [Sproutâ€‘AGI](https://huggingface.co/datasets/ayjays132/Sprout-AGI) dataset. The script compares baseline vs. fineâ€‘tuned perplexity and preserves the better model.
```bash
premium_workflow --sprout-benchmark --model ayjays132/NeuroReasoner-1-NR-1 --prompt "Hello world"
```
Observed on CPU:

| Model | Baseline PPL | Tuned PPL |
|-------|--------------|-----------|
| ayjays132/NeuroReasoner-1-NR-1 | 66.44 | 8.94 |

Prompt **"Hello world"** produced:

| Stage | Continuation |
|-------|--------------|
| Baseline | Hello world] [DIFFICULTY:advanced][3.5/4]. rank; |
| Tuned | Hello world where space exploration is a global project, with diverse stakeholders and technologies to solve complex problems in time.' |

### ğŸ•¹ï¸ Autonomous Gym Training
```bash
premium_workflow --gym --benchmark CartPole-v1
```
Runs a lightweight policy gradient agent inside Gym while language model components continue to respond.

### ğŸ§¬ Evolutionary Learning
To enable background selfâ€‘optimization, attach the evolutionary system to your agent:
```python
from evolutionary_learner import integrate_evolutionary_learning
learning_system = integrate_evolutionary_learning(agent)
```
The system periodically mutates hyperâ€‘parameters, evaluates fitness, and persists the best checkpoints.

## ğŸ›  Development
Run unit tests (requires torch â‰¥2.6 for full pass):
```bash
pytest
```

## ğŸ“„ License
Research use only.
